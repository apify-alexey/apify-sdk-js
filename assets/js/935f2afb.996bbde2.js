"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"label":"Upgrading","type":"link","href":"https://crawlee.dev/docs/upgrading/upgrading-to-v3"}]},"docs":{"examples/accept-user-input":{"id":"examples/accept-user-input","title":"Accept user input","description":"This example accepts and logs user input:"},"examples/add-data-to-dataset":{"id":"examples/add-data-to-dataset","title":"Add data to dataset","description":"This example saves data to the default dataset. If the dataset doesn\'t exist, it will be created."},"examples/basic-crawler":{"id":"examples/basic-crawler","title":"Basic crawler","description":"This is the most bare-bones example of the Apify SDK, which demonstrates some of its building blocks such as the BasicCrawler. You probably don\'t need to go this deep though, and it would be better to start with one of the full-featured crawlers"},"examples/capture-screenshot":{"id":"examples/capture-screenshot","title":"Capture a screenshot using Puppeteer","description":"To run this example on the Apify Platform, select the apify/actor-node-puppeteer-chrome image for your Dockerfile."},"examples/cheerio-crawler":{"id":"examples/cheerio-crawler","title":"Cheerio crawler","description":"This example demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the Cheerio library and extract some data from it: the page title and all h1 tags."},"examples/crawl-all-links":{"id":"examples/crawl-all-links","title":"Crawl all links on a website","description":"This example uses the enqueueLinks() method to add new links to the RequestQueue as the crawler navigates from page to page. If only the"},"examples/crawl-multiple-urls":{"id":"examples/crawl-multiple-urls","title":"Crawl multiple URLs","description":"This example crawls the specified list of URLs."},"examples/crawl-relative-links":{"id":"examples/crawl-relative-links","title":"Crawl a website with relative links","description":"When crawling a website, you may encounter different types of links present that you may want to crawl."},"examples/crawl-single-url":{"id":"examples/crawl-single-url","title":"Crawl a single URL","description":"This example uses the got-scraping npm package"},"examples/crawl-sitemap":{"id":"examples/crawl-sitemap","title":"Crawl a sitemap","description":"This example downloads and crawls the URLs from a sitemap."},"examples/crawl-some-links":{"id":"examples/crawl-some-links","title":"Crawl some links on a website","description":"This CheerioCrawler example uses the pseudoUrls property in the enqueueLinks() method to only add links to the RequestQueue queue if they match the specified regular expression."},"examples/forms":{"id":"examples/forms","title":"Forms","description":"This example demonstrates how to use PuppeteerCrawler to"},"examples/map-and-reduce":{"id":"examples/map-and-reduce","title":"Dataset Map and Reduce methods","description":"This example shows an easy use-case of the Dataset map"},"examples/playwright-crawler":{"id":"examples/playwright-crawler","title":"Playwright crawler","description":"This example demonstrates how to use PlaywrightCrawler"},"examples/puppeteer-crawler":{"id":"examples/puppeteer-crawler","title":"Puppeteer crawler","description":"This example demonstrates how to use PuppeteerCrawler in combination"},"examples/puppeteer-recursive-crawl":{"id":"examples/puppeteer-recursive-crawl","title":"Puppeteer recursive crawl","description":"Run the following example to perform a recursive crawl of a website using PuppeteerCrawler."},"examples/puppeteer-with-proxy":{"id":"examples/puppeteer-with-proxy","title":"Puppeteer with proxy","description":"FIXME: is this staying?"},"examples/use-stealth-mode":{"id":"examples/use-stealth-mode","title":"Use stealth mode","description":"Stealth mode allows you to bypass anti-scraping techniques that use"},"guides/apify-platform":{"id":"guides/apify-platform","title":"Apify Platform","description":"Apify is a platform built to serve large-scale and high-performance web scraping"},"guides/docker-images":{"id":"guides/docker-images","title":"Running in Docker","description":"Running headless browsers in Docker requires a lot of setup to do it right. But you don\'t need to worry about that, because we already did it for you and created base images that you can freely use. We use them every day on the Apify Platform."},"guides/environment-variables":{"id":"guides/environment-variables","title":"Environment Variables","description":"The following is a list of the environment variables used by Apify SDK that are available to the user."},"guides/getting-started":{"id":"guides/getting-started","title":"Getting Started","description":"Without the right tools, crawling and scraping the web can be difficult. At the very least, you need an HTTP client to make the necessary"},"guides/motivation":{"id":"guides/motivation","title":"Motivation","description":"Thanks to tools like Playwright, Puppeteer or"},"guides/proxy-management":{"id":"guides/proxy-management","title":"Proxy Management","description":"IP address blocking is one of the oldest"},"guides/quick-start":{"id":"guides/quick-start","title":"Quick Start","description":"This short tutorial will set you up to start using Apify SDK in a minute or two."},"guides/request-storage":{"id":"guides/request-storage","title":"Request Storage","description":"The Apify SDK has several request storage types that are useful for specific tasks. The requests are stored either on local disk to a directory defined by the"},"guides/result-storage":{"id":"guides/result-storage","title":"Result Storage","description":"The Apify SDK has several result storage types that are useful for specific tasks. The data is stored either on local disk to a directory defined by the"},"guides/session-management":{"id":"guides/session-management","title":"Session Management","description":"&#8203;SessionPool is a class that allows you to handle the rotation of proxy IP addresses along with cookies and other custom settings in Apify SDK."},"guides/type-script-actor":{"id":"guides/type-script-actor","title":"TypeScript Actors","description":"Apify SDK supports TypeScript by covering public APIs with type declarations. This"},"readme/introduction":{"id":"readme/introduction","title":"Apify SDK: The scalable web crawling and scraping library for JavaScript","description":"npm version"},"readme/overview":{"id":"readme/overview","title":"overview","description":"Overview"},"readme/support":{"id":"readme/support","title":"support","description":"Support"},"upgrading/upgrading-to-v1":{"id":"upgrading/upgrading-to-v1","title":"Upgrading to v1","description":"Summary"},"upgrading/upgrading-to-v2":{"id":"upgrading/upgrading-to-v2","title":"Upgrading to v2","description":"- BREAKING: Require Node.js >=15.10.0 because HTTP2 support on lower Node.js versions is very buggy."},"upgrading/upgrading-to-v3":{"id":"upgrading/upgrading-to-v3","title":"Upgrading to v3","description":"- TS rewrite"}}}')}}]);